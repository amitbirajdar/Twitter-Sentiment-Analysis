{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProblemSet5_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyQJ-Ui-coZr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktznA0s0Q90a"
      },
      "source": [
        "# importing required libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import models, Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, GlobalAveragePooling1D, Input, SpatialDropout1D\n",
        "import string\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.regularizers import l1, l2\n",
        "from sklearn.metrics import accuracy_score, f1_score, RocCurveDisplay, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO_BiHnovbLq"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dstI4paTjOo"
      },
      "source": [
        "# importing data\n",
        "# data -> tweets\n",
        "# labels -> sentiment\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Problem Set 5/ps5_tweets_text.csv')\n",
        "labels = pd.read_csv('/content/drive/MyDrive/Problem Set 5/ps5_tweets_labels_as_numbers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVo0cvzeVFgg"
      },
      "source": [
        "# merging tweets and labels into one dataframe\n",
        "\n",
        "fulldata = data.merge(labels, right_on='Id', left_on='Id')\n",
        "fulldata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVVhJanSTuxV"
      },
      "source": [
        "label_meanings = {0:\"Extremely Negative\", 1:\"Negative\", 2:\"Neutral\", 3:\"Positive\", 4:\"Extremely Positive\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NNr3klwvf_b"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmMVvkNx6ZE7"
      },
      "source": [
        "# plotting bar chart number of tweets for each sentiment class\n",
        "# plotting pie chart for showing percentage of tweets for each sentiment class\n",
        "\n",
        "x0 = fulldata.Label.value_counts().sort_index()[0]\n",
        "x1 = fulldata.Label.value_counts().sort_index()[1]\n",
        "x2 = fulldata.Label.value_counts().sort_index()[2]\n",
        "x3 = fulldata.Label.value_counts().sort_index()[3]\n",
        "x4 = fulldata.Label.value_counts().sort_index()[4]\n",
        "\n",
        "x = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"]\n",
        "y = [x0, x1, x2, x3, x4]\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Bar Plot\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Number of tweets for each category')\n",
        "plt.bar(x,y)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Number of tweets')\n",
        "\n",
        "# Pie Chart\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Percentage of tweets for each category')\n",
        "\n",
        "plt.pie(y, colors=None, labels=x, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=30, wedgeprops={'alpha':0.6})\n",
        "plt.axis('equal') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJUq34_4c56-"
      },
      "source": [
        "# calculating mean tweet length for each sentiment\n",
        "\n",
        "def avgtweetlength(sentiment, corpus):\n",
        "\n",
        "  # sentiment -> specifying the sentiment class\n",
        "  # corpus -> collection of all tweets for a particular sentiment\n",
        "\n",
        "  # creating a dataframe to store sentiment specific tweets and corresponding lengths\n",
        "  df = pd.DataFrame()\n",
        "  df[sentiment] = corpus\n",
        "  df['len'] = df[sentiment].apply(lambda x: len(x))\n",
        "\n",
        "  # calculating mean length\n",
        "  l = df['len'].mean()\n",
        "  print(sentiment+' =', round(l,2))\n",
        "  \n",
        "  return round(l,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qPqHlf_c9Hg"
      },
      "source": [
        "# Plotting mean tweet length for each sentiment class\n",
        "\n",
        "print('Average tweet lengths for each sentiment')\n",
        "y = []\n",
        "for i in range(5):\n",
        "  data = fulldata['Tweet'][fulldata['Label']==i]\n",
        "  data.reset_index(drop=True, inplace=True)\n",
        "  y.append(avgtweetlength(x[i], data))\n",
        "\n",
        "plt.figure()\n",
        "plt.title('Average tweet length for each sentiment')\n",
        "plt.bar(x,y, color=['red','orange','blue','yellow','green'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Average number of tweets')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0BqZ5t3dAsP"
      },
      "source": [
        "# WordCloud\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "alltweets = ''\n",
        "\n",
        "for tweet in fulldata.Tweet:\n",
        "  alltweets = alltweets + tweet + ' '\n",
        "\n",
        "alltweets = alltweets.strip()\n",
        "\n",
        "plt.figure(figsize=(15,15))\n",
        "list_of_stopwords = set(STOPWORDS)\n",
        "list_of_stopwords.update(['https', 'co','com', 'amp', 'will'])\n",
        "\n",
        "wordcloud = WordCloud(stopwords=list_of_stopwords, background_color='white').generate(alltweets)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_b50O_a11Cs"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Cleaning the tweets\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsBtTmvqq0yx"
      },
      "source": [
        "# cleaning the text\n",
        "\n",
        "def clean(tweet):\n",
        "  tr = ''\n",
        "\n",
        "  # remove hyperlinks\n",
        "  tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n",
        "  \n",
        "  # remove punctuations and replace with space\n",
        "  for char in tweet:\n",
        "    if char not in string.punctuation:\n",
        "      tr = tr + char\n",
        "    else:\n",
        "      tr = tr + ' '\n",
        "  tweet = tr\n",
        "\n",
        "  # remove digits\n",
        "  tweet = re.sub('[0-9]+', '', tweet)\n",
        "\n",
        "  # return clean text\n",
        "  return tweet\n",
        "\n",
        "fulldata['Tweet_cleaned'] = fulldata['Tweet'].apply(lambda x: clean(x))\n",
        "fulldata.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTN_Zz9fdf9o"
      },
      "source": [
        "# removing words that are one character long\n",
        "\n",
        "def onechar(line):\n",
        "  \n",
        "  words = line.split(' ')\n",
        "  b = ''\n",
        "  for word in words:\n",
        "    if len(word) >= 2:\n",
        "      b = b + word + ' '\n",
        "\n",
        "  return b.strip()\n",
        "fulldata['Tweet_cleaned'] = fulldata['Tweet_cleaned'].apply(lambda x: onechar(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ67SGX5s-EX"
      },
      "source": [
        "# loading stopwords from nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxoG-B3-ts2W"
      },
      "source": [
        "# removing stopwords from the tweets\n",
        "\n",
        "def rem_stopwords(tweet):\n",
        "  tr, new = '', ''\n",
        "  tweet = tweet + ' '\n",
        "  for char in tweet:\n",
        "    if char != ' ':\n",
        "      tr = tr + char\n",
        "    else:\n",
        "      if tr not in stopwords:\n",
        "        new = new + tr + ' '\n",
        "      tr = ''\n",
        "  new = new.strip()\n",
        "  new = new.lower()\n",
        "  return new\n",
        "    \n",
        "fulldata['Tweet_wostop'] = fulldata['Tweet_cleaned'].apply(lambda x: rem_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjPBMziidqn-"
      },
      "source": [
        "# Performing stemming on cleaned tweets\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# stemming\n",
        "\n",
        "def stemming(line):\n",
        "  \n",
        "  a = ''\n",
        "  words = line.split()\n",
        "  for word in words:\n",
        "    a = a + stemmer.stem(word) + ' '\n",
        "\n",
        "  return a.strip()\n",
        "fulldata['Tweet_wostop'] = fulldata['Tweet_wostop'].apply(lambda x: onechar(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWgDlPfpd0-E"
      },
      "source": [
        "# removing escape characters\n",
        "\n",
        "def escape(line):\n",
        "  escapes = ''.join([chr(char) for char in range(1, 32)])\n",
        "  \n",
        "  translator = str.maketrans('', '', escapes)\n",
        "  b = line.translate(translator)\n",
        "  return line.translate(translator)\n",
        "\n",
        "fulldata['Tweet_wostop'] = fulldata['Tweet_wostop'].apply(lambda x: escape(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diq_bcsMwNpt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "Final Cleaned Data\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrHfDI2Adu-b"
      },
      "source": [
        "fulldata[['Id','Tweet', 'Tweet_wostop','Label']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i7__Q05ucaP"
      },
      "source": [
        "# Randomly shuffling the dataframe\n",
        "\n",
        "fulldata = shuffle(fulldata)\n",
        "fulldata.reset_index(drop=True, inplace=True)\n",
        "fulldata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2c2yt9u3ujK"
      },
      "source": [
        "# checking the length of a tweet before and after removing stop words and escape characters\n",
        "\n",
        "len(fulldata.Tweet[0]), len(fulldata.Tweet_wostop[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU4xBuQzwoDQ"
      },
      "source": [
        "# Preparing Data for feeding it to the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFZSxdY3T__9"
      },
      "source": [
        "# splitting data into cleaned tweets and corresponding labels\n",
        "\n",
        "x = fulldata.Tweet_wostop # clean tweet data\n",
        "y = fulldata.Label # labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tz8XjGNf-Gu"
      },
      "source": [
        "# splitting full data into training and testing \n",
        "\n",
        "train_len = int(x.shape[0]*0.8) # 80% of the data is for training, 20% for testing\n",
        "\n",
        "# training data\n",
        "trainX = x[:train_len]\n",
        "trainY = y[:train_len]\n",
        "\n",
        "# testing data\n",
        "testX = x[train_len:]\n",
        "testY = y[train_len:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLQ4HI9zWSDZ"
      },
      "source": [
        "# splitting training data into train and validation sets\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(trainX, trainY, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc91apMB92fQ"
      },
      "source": [
        "x_train.shape, y_train.shape, x_val.shape, y_val.shape, testX.shape, testY.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi5FSEXYYRj7"
      },
      "source": [
        "# Tokenizing tweets\n",
        "\n",
        "max_words = 10000 # max words to consider in the vocabulary\n",
        "max_len = 55 # max number of words to consider in a tweet\n",
        "tokenizer = Tokenizer(num_words = max_words) # creating Tokenizer object\n",
        "tokenizer.fit_on_texts(x_train) # fitting tokenizer on the train set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1stGaoYdW5A"
      },
      "source": [
        "# preparing sequences by substituting tokens with their corresponding integer values \n",
        "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
        "\n",
        "# padding the sequences to make all tweets of same length\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len)#, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFPtY5Xz6PfW"
      },
      "source": [
        "train_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvAP4Ih_Bonv"
      },
      "source": [
        "# tokenizing and creating padded sequences for validation set\n",
        "val_sequences = tokenizer.texts_to_sequences(x_val)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_len)#, padding='post')\n",
        "\n",
        "# tokenizing and creating padded sequences for test set\n",
        "test_sequences = tokenizer.texts_to_sequences(testX)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len)#, padding='post')\n",
        "\n",
        "# default padding is done before (if not specified explicitly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zg1G8eZ76T_z"
      },
      "source": [
        "val_padded.shape, test_padded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMolYoc7TpK4"
      },
      "source": [
        "# padded sequence example\n",
        "train_padded[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8-toiUmypgh"
      },
      "source": [
        "# Training the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjjLWCGUdlDU"
      },
      "source": [
        "# preparing the model\n",
        "\n",
        "model = Sequential([\n",
        "                    #Input(name='inputs', shape=[max_len]),\n",
        "\n",
        "                    Embedding(max_words, 60, input_length = max_len),\n",
        "                    #GlobalAveragePooling1D(),\n",
        "                    LSTM(128, recurrent_dropout=0.3),#, return_sequences=True),\n",
        "                    \n",
        "                    # Dense(128, activation='tanh'),#, kernel_regularizer=l2(0.01)),\n",
        "                    # Dropout(0.5),\n",
        "                    #GlobalAveragePooling1D(),\n",
        "                    # Dense(64, activation='relu'),\n",
        "                    # Dropout(0.5),\n",
        "                    # Dense(64, activation='relu'),\n",
        "                    # Dropout(0.5),\n",
        "                    Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdM4RB0--03P"
      },
      "source": [
        "# compiling the model\n",
        "\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "opt = Adam(0.0001)\n",
        "sgd = SGD(lr = 0.0001)\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QmTD7fIAChu"
      },
      "source": [
        "# training the model\n",
        "\n",
        "# specifying callback to stop when validation loss keeps increasing for 4 straight epochs\n",
        "callback = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=4, verbose=0, mode='auto', restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_padded, y_train, epochs=10, batch_size=32, validation_data=(val_padded, y_val), callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9CubFzHdmN6"
      },
      "source": [
        "# plotting the accuracy and loss for training and validation\n",
        "\n",
        "legend = ['validation', 'train']\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "# plotting accuracy\n",
        "plt.subplot(1,2,1)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(legend)\n",
        "\n",
        "# plotting loss\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('Loss')\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(legend)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWxVe9fgZw4R"
      },
      "source": [
        "# evaluating model performance on test set\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_padded, testY)\n",
        "\n",
        "print('Test Loss = ', test_loss)\n",
        "print(f'Test Accuracy = {round(test_accuracy*100, 2)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGDmuCeakjRP"
      },
      "source": [
        "# predicting labels for test set\n",
        "\n",
        "y_predf1 = model.predict_proba(test_padded)\n",
        "y_predf1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYkGigSHcPV4"
      },
      "source": [
        "'''Plotting ROC AUC curves for Test Set'''\n",
        "\n",
        "'''Referred from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html'''\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_curve,roc_auc_score,auc \n",
        "from sklearn import preprocessing\n",
        "from itertools import cycle\n",
        "\n",
        "def plotrocauc(model):\n",
        "    \n",
        "    y_pred = model.predict(test_padded)\n",
        "    y_predf1 = model.predict_proba(test_padded)\n",
        "    \n",
        "    #print(testY)\n",
        "\n",
        "    macro_roc_auc_ovo = roc_auc_score(testY, y_pred, multi_class=\"ovo\",average=\"macro\")\n",
        "    weighted_roc_auc_ovo = roc_auc_score(testY, y_pred, multi_class=\"ovo\",average=\"weighted\")\n",
        "    macro_roc_auc_ovr = roc_auc_score(testY, y_pred, multi_class=\"ovr\",average=\"macro\")\n",
        "    weighted_roc_auc_ovr = roc_auc_score(testY, y_pred, multi_class=\"ovr\",average=\"weighted\")\n",
        "    \n",
        "    \n",
        "    print(\"One-vs-One-> {:.6f} (weighted by prevalence)\".format(weighted_roc_auc_ovo))\n",
        "    print(\"One-vs-Rest-> {:.6f} (weighted by prevalence)\".format(weighted_roc_auc_ovr))\n",
        "\n",
        "    lb = preprocessing.LabelBinarizer()\n",
        "    lb.fit(testY)\n",
        "    y_test = lb.transform(testY)\n",
        "    \n",
        "    n_classes=5\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    \n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure(figsize=(7,7))\n",
        "    \n",
        "    colors = cycle(['blue', 'red', 'green', 'orange', 'yellow'])\n",
        "    \n",
        "    for i, color in zip(range(n_classes), colors):\n",
        "        plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.format(i, roc_auc[i]))\n",
        "    \n",
        "    plt.plot([0, 1], [0, 1], 'k-', lw=1.5)\n",
        "    plt.xlim([-0.05, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNYUt1xwcZCl"
      },
      "source": [
        "# plotting ROC curve\n",
        "\n",
        "plotrocauc(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNNOU_oKHmvb"
      },
      "source": [
        "ypred = model.predict_classes(test_padded)\n",
        "ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc0jsG3kHYi9"
      },
      "source": [
        "# plotting confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "label_meanings = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\",\"Extremely Positive\"]\n",
        "conf_mat = confusion_matrix(testY, ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.title('Confusion matrix for LSTM')\n",
        "plot = sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plot.set_ylabel('Actual', fontsize=15)\n",
        "plot.set_xlabel('Predicted', fontsize=15)\n",
        "plot.set_xticklabels(label_meanings)\n",
        "#plot.set_yticklabels(label_meanings)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTu5j0ey0Ml"
      },
      "source": [
        "# TF-IDF vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMFbP0cqy3lQ"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz3b0knNy567"
      },
      "source": [
        "# corpus is a list of all tweets\n",
        "\n",
        "corpus = fulldata.Tweet_wostop.to_list()\n",
        "len(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY_UDOmSy53c"
      },
      "source": [
        "# tfidf vectorizing the corpus\n",
        "\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words='english')\n",
        "x = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zhvzM0Hy50y"
      },
      "source": [
        "# splitting into train and test data\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, fulldata.Label, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLDhKWbIyuBd"
      },
      "source": [
        "# Multinomial Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4PUuPdCy5yj"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb = MultinomialNB()\n",
        "\n",
        "# fitting the model on train data\n",
        "model2 = nb.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a37QtOsUzDSN"
      },
      "source": [
        "# generating predictions on test data\n",
        "ypred = model2.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aVQMeKUzDPP"
      },
      "source": [
        "ypred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X4uh6n-zDLv"
      },
      "source": [
        "# calculating accuracy and f1 score for the model\n",
        "\n",
        "acc = round(accuracy_score(y_test, ypred),2)\n",
        "auc = round(roc_auc_score(y_test, model2.predict_proba(X_test), multi_class='ovr'),2)\n",
        "\n",
        "print(f'Accuracy = {acc*100} %')\n",
        "print(f'ROC_AUC_Score = {auc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zFbx8kXzYYm"
      },
      "source": [
        "# plotting confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "label_meanings = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\",\"Extremely Positive\"]\n",
        "conf_mat = confusion_matrix(y_test, ypred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.title('Confusion matrix for Multinomial Naive Bayes')\n",
        "plot = sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plot.set_ylabel('Actual', fontsize=15)\n",
        "plot.set_xlabel('Predicted', fontsize=15)\n",
        "plot.set_xticklabels(label_meanings)\n",
        "#plot.set_yticklabels(label_meanings)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o39mQabSzQBz"
      },
      "source": [
        "# Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsPEx4GOzDKF"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "log = LogisticRegression(max_iter=500, solver='liblinear', multi_class='auto')\n",
        "\n",
        "# fitting the model to training data\n",
        "logreg = log.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9txdeADoyyKA"
      },
      "source": [
        "# predicting accuracy and f1 score for the model\n",
        "logpred = logreg.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So6dvYu-7Tpk"
      },
      "source": [
        "# calculating accuracy and f1 score for the model\n",
        "\n",
        "acc = round(accuracy_score(y_test, logpred),2)\n",
        "auc = round(roc_auc_score(y_test, logreg.predict_proba(X_test), multi_class='ovr'),2)\n",
        "\n",
        "print(f'Accuracy = {acc*100} %')\n",
        "print(f'ROC_AUC_Score = {auc}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTwdyB1qzWCE"
      },
      "source": [
        "# plotting confusion matrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "label_meanings = [\"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\",\"Extremely Positive\"]\n",
        "conf_mat = confusion_matrix(y_test, logpred)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "plt.title('Confusion matrix for Logistic Regression')\n",
        "plot = sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plot.set_ylabel('Actual', fontsize=15)\n",
        "plot.set_xlabel('Predicted', fontsize=15)\n",
        "plot.set_xticklabels(label_meanings)\n",
        "#plot.set_yticklabels(label_meanings)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}